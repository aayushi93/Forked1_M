{"paragraphs":[{"text":"%md\n# Creating Dataframes\n\nSpark DF allows you to create dataframes from the following sources.\n\n- Option 1: Spark Sequence (built-in data structure)\n- Option 2: External sources\n    - Hive tables (connect to Hive metastore)\n    - Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)","user":"anonymous","dateUpdated":"2020-03-09T14:02:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Dataframes</h1>\n<p>Spark DF allows you to create dataframes from the following sources.</p>\n<ul>\n  <li>Option 1: Spark Sequence (built-in data structure)</li>\n  <li>Option 2: External sources\n    <ul>\n      <li>Hive tables (connect to Hive metastore)</li>\n      <li>Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151367_-1592960110","id":"20200119-073223_398987810","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:02:59+0000","dateFinished":"2020-03-09T14:03:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:395"},{"text":"%md\n## Option 1: Spark Sequence\n\nSpark DF can convert a sequence of tuples to Spark DF.\ne.g., `Seq[(String, Double, String, String)]`\n\n- A tuple corresponds to a DF row.\n- An element in a tuple corresponds to a column to a particular row.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-03-09T14:03:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 1: Spark Sequence</h2>\n<p>Spark DF can convert a sequence of tuples to Spark DF.<br/>e.g., <code>Seq[(String, Double, String, String)]</code></p>\n<ul>\n  <li>A tuple corresponds to a DF row.</li>\n  <li>An element in a tuple corresponds to a column to a particular row.</li>\n</ul>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151378_2033379714","id":"20190519-201210_1157722001","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:02+0000","dateFinished":"2020-03-09T14:03:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"text":"%md\n### Scala Implicit Conversions (optional)\n\nIn short, you have to `import spark.implicits._` to convert/cast a `Seq[(String, Double, String, String)]` to a Spark `DataFrame`. (e.g. `lineTupleSeq.toDF`)\n\nThis is called implicit conversions in Scala. In this case, `spark.implicits.localSeqToDatasetHolder` creates a Dataset from a local Seq.\n\nSpark Scala Docs:\n\n- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a>\n- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a>","user":"anonymous","dateUpdated":"2020-03-09T14:03:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Scala Implicit Conversions (optional)</h3>\n<p>In short, you have to <code>import spark.implicits._</code> to convert/cast a <code>Seq[(String, Double, String, String)]</code> to a Spark <code>DataFrame</code>. (e.g. <code>lineTupleSeq.toDF</code>)</p>\n<p>This is called implicit conversions in Scala. In this case, <code>spark.implicits.localSeqToDatasetHolder</code> creates a Dataset from a local Seq.</p>\n<p>Spark Scala Docs:</p>\n<ul>\n  <li>\n  <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a></li>\n  <li>\n  <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151379_2130565053","id":"20190520-102917_1809142825","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:02+0000","dateFinished":"2020-03-09T14:03:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:397"},{"text":"/**\n * Each line/record/row must be a Tuple\n * e.g.  Tuple(AAPL,110.5,2018-02-01,Apple)\n * \n * Lines are grouped into a Seq\n * List(\n *   (AAPL,110.5,2018-02-01,Apple),\n *   (AMZN,1500.52,2018-02-01,Amazon.com),\n *   (FB,170.01,2018-02-01,Facebook)\n * )\n */\nval lineTuple1 = (\"AAPL\",110.5,\"2018-02-01\",\"Apple\")\nval lineTuple2 = (\"AMZN\",1500.52,\"2018-02-01\",\"Amazon.com\")\nval lineTuple3 = (\"FB\",170.01,\"2018-02-01\",\"Facebook\")\nval lineTupleSeq = Seq(lineTuple1,lineTuple2,lineTuple3)\n\n//To use toDF, you must import this (see next section for details)\n//In fact Zeppellin interpreter already imported this for you\nimport spark.implicits._\nval stockDf = lineTupleSeq.toDF(\"ticker\",\"price\", \"date\", \"companyName\")\nprintln(\">>Print Schema\")\nstockDf.printSchema\n\n//SELECT * FROM stock LIMIT 3\nprintln(\">>Print 3 rows\")\nstockDf.show(3)\n\n//SELECT companyName AS company_name, price FROM stock\nprintln(\">>Print all rows with renamed columns\")\nstockDf.select(col(\"companyName\").as(\"company_name\"), col(\"price\").as(\"bid_price\")).show()\n\n//Use SQL to query dataframe\n//creating a tmep view\nstockDf.createOrReplaceTempView(\"stock_view\")\n//execute query\nval filteredDf = spark.sql(\"SELECT * FROM stock_view WHERE price > 100.0\")\nprintln(\">>SQL\")\nfilteredDf.show()\n\n","user":"anonymous","dateUpdated":"2020-03-09T14:03:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lineTuple1: (String, Double, String, String) = (AAPL,110.5,2018-02-01,Apple)\nlineTuple2: (String, Double, String, String) = (AMZN,1500.52,2018-02-01,Amazon.com)\nlineTuple3: (String, Double, String, String) = (FB,170.01,2018-02-01,Facebook)\nlineTupleSeq: Seq[(String, Double, String, String)] = List((AAPL,110.5,2018-02-01,Apple), (AMZN,1500.52,2018-02-01,Amazon.com), (FB,170.01,2018-02-01,Facebook))\nimport spark.implicits._\nstockDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\n>>Print Schema\nroot\n |-- ticker: string (nullable = true)\n |-- price: double (nullable = false)\n |-- date: string (nullable = true)\n |-- companyName: string (nullable = true)\n\n>>Print 3 rows\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01| Amazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n>>Print all rows with renamed columns\n+------------+---------+\n|company_name|bid_price|\n+------------+---------+\n|       Apple|    110.5|\n|  Amazon.com|  1500.52|\n|    Facebook|   170.01|\n+------------+---------+\n\nfilteredDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\n>>SQL\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01| Amazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=0","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151380_-784691047","id":"20190519-201416_412351679","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:03+0000","dateFinished":"2020-03-09T14:03:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:398"},{"text":"%md\n## Option 2: External sources\nThe most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-03-09T14:03:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 2: External sources</h2>\n<p>The most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.</p>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151381_-888244655","id":"20190520-104920_1833330750","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:16+0000","dateFinished":"2020-03-09T14:03:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:399"},{"text":"//Read CSV file to df\n//local or hdfs path\nval path = \"/user/vassairm/datasets/online_retail/online-retail-dataset.txt\"\n\n//spark.read is able to handle csv formats\nval retailDf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path)\n\nretailDf.printSchema\nretailDf.show(3)\nretailDf.show(3,false)\nretailDf.sample(false, 0.2).sort($\"InvoiceDate\".desc).show(3)","user":"anonymous","dateUpdated":"2020-03-09T14:03:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = /user/vassairm/datasets/online_retail/online-retail-dataset.txt\nretailDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|  InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\n|   566079|    22403|MAGNETS PACK OF 4...|      48|9/9/2011 9:52|     0.39|     17593|United Kingdom|\n|   566079|    23432|PRETTY HANGING QU...|      72|9/9/2011 9:52|     0.83|     17593|United Kingdom|\n|   566079|    23298|      SPOTTY BUNTING|      12|9/9/2011 9:52|     4.95|     17593|United Kingdom|\n+---------+---------+--------------------+--------+-------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=2","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=3","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=4","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=5","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=6"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151381_1153085980","id":"20190520-095229_630927102","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:17+0000","dateFinished":"2020-03-09T14:03:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:400"},{"text":"%md\n### Column Type Cast\nIn the previous paragraph, the data type of the `InvoiceDate` column is String instead of `timestamp`. In this practice, you need to cast `InvoiceDate` column to Spark `timestamp` data type.\n\n```bash\nresultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n```\n","user":"anonymous","dateUpdated":"2020-03-09T14:03:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Column Type Cast</h3>\n<p>In the previous paragraph, the data type of the <code>InvoiceDate</code> column is String instead of <code>timestamp</code>. In this practice, you need to cast <code>InvoiceDate</code> column to Spark <code>timestamp</code> data type.</p>\n<pre><code class=\"bash\">resultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151382_2070583147","id":"20190520-085947_2007764287","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:36+0000","dateFinished":"2020-03-09T14:03:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:401"},{"text":"//write code to cast InvoiceData from String to timestamp\n//save the result DF as 'val retailCastDf'\n//Date format appears to be month/day/year hour:minute\nval retailCastDf = retailDf.withColumn(\"InvoiceDate\",to_timestamp($\"InvoiceDate\",\"MM/dd/yyyy HH:mm\"))\n\n//print schema\nretailCastDf.printSchema\n//print rows to verify\nretailCastDf.show(3)\n//Cache DF in memory since it will be accessed frequently\nretailCastDf.cache","user":"anonymous","dateUpdated":"2020-03-09T14:03:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailCastDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\nres38: retailCastDf.type = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=7"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151383_-245709097","id":"20190519-215300_721200493","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:36+0000","dateFinished":"2020-03-09T14:03:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:402"},{"text":"%md\n# DF Operations","user":"anonymous","dateUpdated":"2020-03-09T14:03:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Operations</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151383_-193039264","id":"20191010-103454_2098588366","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:39+0000","dateFinished":"2020-03-09T14:03:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:403"},{"text":"%md\n## DataFrame SELECT\nImplement the following SQL queries using dataframe. Compare different select syntax.\n\n```sql\nSELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n```","user":"anonymous","dateUpdated":"2020-03-09T14:03:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame SELECT</h2>\n<p>Implement the following SQL queries using dataframe. Compare different select syntax.</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151385_2060250017","id":"20190519-221054_1925024171","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:39+0000","dateFinished":"2020-03-09T14:03:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:404"},{"text":"//SELECT * from retail limit 3;\nretailCastDf.show(3)\nretailCastDf.limit(3).show\n\nimport org.apache.spark.sql.functions._\n// SELECT InvoiceNo FROM retail;\n//Different ways of selecting a column\nretailCastDf.select(\"InvoiceNo\").show(1) // String column name\n\n//Column reference shorthand styles\nretailCastDf.select($\"InvoiceNo\").show(1)\nretailCastDf.select('InvoiceNo).show(1)\n\n//Functions to retrieve columns\nretailCastDf.select(col(\"InvoiceNo\")).show(1)\nretailCastDf.select(retailCastDf.col(\"InvoiceNo\")).show(1)\n\n//Expression and pure SQL\nretailCastDf.select(expr(\"InvoiceNo\")).show(1)\nretailCastDf.createOrReplaceTempView(\"retail\") // Create a temp view...\nspark.sql(\"SELECT InvoiceNo FROM retail\").show(1) // ... and query with SQL\n\n//ERROR: cannot mix types\n//retailCastDf.select($\"InvoiceNo\", \"StockCode\").show(1)\n\n//Using both column shorthands in one function is fine\n//retailCastDf.select('InvoiceNo, $\"InvoiceDate\").show(1)\n\n//expr or selectExpr is most powerful and close to SQL syntax\n//SELECT InvoiceNo as invoiceId from retail limit 1;\n//col(\"InvoiceNo\").as(\"invoiceId\")\nretailCastDf.select(expr(\"InvoiceNo as invoiceId\")).show(1)\nretailCastDf.selectExpr(\"InvoiceNo as invoiceId\").show(1)\n\n//SELECT * from retail limit 1;\nretailCastDf.selectExpr(\"*\").show(1)\n\n//select max(UnitPrice) as maxUnitPrice from retail\nretailCastDf.selectExpr(\"max(UnitPrice) as maxUnitPrice\").show \n","user":"anonymous","dateUpdated":"2020-03-09T15:42:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\nimport org.apache.spark.sql.functions._\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\n+------------+\n|maxUnitPrice|\n+------------+\n|     38970.0|\n+------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=8","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=9","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=10","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=11","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=12","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=13","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=14","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=15","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=16","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=17","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=18","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=19","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=20"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151386_-275860502","id":"20190519-211701_1956303781","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:40+0000","dateFinished":"2020-03-09T14:03:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:405"},{"text":"%md\n## DataFrame filtering (WHERE)\n\nImplement the following SQL query\n\n```sql\nSELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n```\n\nSample results\n```\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n```","user":"anonymous","dateUpdated":"2020-03-09T14:03:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame filtering (WHERE)</h2>\n<p>Implement the following SQL query</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n</code></pre>\n<p>Sample results</p>\n<pre><code>+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151386_665562452","id":"20190519-221114_648626738","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:56+0000","dateFinished":"2020-03-09T14:03:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:406"},{"text":"\n\n// SELECT * FROM retail WHERE InvoiceNo = 536365 LIMIT 2;\nretailCastDf.where('InvoiceNo === 536365).limit(2).show","user":"anonymous","dateUpdated":"2020-03-09T14:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=21"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151387_1233080929","id":"20190519-201625_2028882244","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:56+0000","dateFinished":"2020-03-09T14:03:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:407"},{"user":"anonymous","dateUpdated":"2020-03-09T14:03:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583437151388_-138425122","id":"20191007-145852_244125478","dateCreated":"2020-03-05T19:39:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:408"},{"text":"%md\n\n# DF Exercises\nIn the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution. ","user":"anonymous","dateUpdated":"2020-03-09T14:03:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Exercises</h1>\n<p>In the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151391_-861614308","id":"20190520-123428_698724288","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:58+0000","dateFinished":"2020-03-09T14:03:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:409"},{"text":"//Spark SQL exercies\n//register retailCastDf as `retail` view\nretailCastDf.createOrReplaceTempView(\"retail\")\n//test the retail temp view\nspark.sql(\"SELECT * FROM retail limit 10\").show","user":"anonymous","dateUpdated":"2020-03-09T15:19:24+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   563016|    21544|SKULLS  WATER TRA...|      12|2011-08-11 12:44:00|     0.85|     15358|United Kingdom|\n|   563016|    22931|BAKING MOULD HEAR...|       6|2011-08-11 12:44:00|     2.55|     15358|United Kingdom|\n|   563016|    22585|PACK OF 6 BIRDY G...|      12|2011-08-11 12:44:00|     1.25|     15358|United Kingdom|\n|   563016|    21497|FANCY FONTS BIRTH...|      25|2011-08-11 12:44:00|     0.42|     15358|United Kingdom|\n|   563016|    23191|BUNDLE OF 3 RETRO...|      12|2011-08-11 12:44:00|     1.65|     15358|United Kingdom|\n|   563016|    23167|SMALL CERAMIC TOP...|      12|2011-08-11 12:44:00|     0.83|     15358|United Kingdom|\n|   563016|    22993|SET OF 4 PANTRY J...|      12|2011-08-11 12:44:00|     1.25|     15358|United Kingdom|\n|   563016|    22817|  CARD SUKI BIRTHDAY|      12|2011-08-11 12:44:00|     0.42|     15358|United Kingdom|\n|   563016|    22961|JAM MAKING SET PR...|      12|2011-08-11 12:44:00|     1.45|     15358|United Kingdom|\n|   563016|    22494|EMERGENCY FIRST A...|      12|2011-08-11 12:44:00|     1.25|     15358|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=22"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151392_-1483339089","id":"20190520-142038_1683726413","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:03:59+0000","dateFinished":"2020-03-09T14:04:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"%md\n#### Q1: Find the top N largest invoices by the amount (`Quantity * UnitPrice`)\n\nNote: `InvoiceNo` will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)\n\n**Sample output**\n```bash\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n```","user":"anonymous","dateUpdated":"2020-03-09T14:04:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q1: Find the top N largest invoices by the amount (<code>Quantity * UnitPrice</code>)</h4>\n<p>Note: <code>InvoiceNo</code> will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)</p>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151393_1907340428","id":"20190520-133812_405266917","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:04:00+0000","dateFinished":"2020-03-09T14:04:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:411"},{"text":"//Find top largest invoices by total cost\n\n//Group by InvoiceNo, sum up total cost of items per InvoiceNo. Sort by amount.\nval largestInvoices = retailCastDf.groupBy($\"InvoiceNo\").agg(sum('Quantity * 'UnitPrice).as(\"Amount\")).orderBy('Amount.desc)\nlargestInvoices.show(5)\n\n//verification using SparkSQL\nspark.sql(\"SELECT InvoiceNo, SUM(Quantity * UnitPrice) AS Amount FROM retail GROUP BY InvoiceNo ORDER BY Amount DESC\").show(5)\n\n\n","user":"anonymous","dateUpdated":"2020-03-09T14:04:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"largestInvoices: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double]\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=23","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=24"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151393_1595414098","id":"20190519-215312_1016690251","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:04:00+0000","dateFinished":"2020-03-09T14:04:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:412"},{"user":"anonymous","dateUpdated":"2020-03-09T14:04:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583437151399_-1277442245","id":"20191007-145909_914572499","dateCreated":"2020-03-05T19:39:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:413"},{"text":"%md\n#### Q2: Find the top N largest invoices by the amount and show receipt details\n\n```\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n```","user":"anonymous","dateUpdated":"2020-03-09T14:04:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q2: Find the top N largest invoices by the amount and show receipt details</h4>\n<pre><code>+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151399_2005977943","id":"20190520-124355_215736883","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:04:12+0000","dateFinished":"2020-03-09T14:04:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:414"},{"text":"//Find top largest invoices by amount, show details\n//Start by getting a table with one line per invoice\nval invoiceInfo = retailCastDf.dropDuplicates(\"InvoiceNo\")\n//Join the above table to the Largest Invoices table we made earlier and select relevant info\nval largeInvoiceInfo = largestInvoices.join(invoiceInfo, largestInvoices(\"InvoiceNo\") === invoiceInfo(\"InvoiceNo\")).select(largestInvoices(\"InvoiceNo\"), 'Amount, 'InvoiceDate, 'CustomerID, 'Country).orderBy('Amount.desc)\nlargeInvoiceInfo.show(5)\n\n//Join the retail table to a subquery getting the largest invoices, and only keep distinct rows.\nspark.sql(\"select distinct a.InvoiceNo, a.Amount, b.InvoiceDate, b.CustomerID, b.Country from (select InvoiceNo, sum(Quantity * UnitPrice) as Amount from retail group by InvoiceNo) as a inner join retail as b on a.InvoiceNo = b.InvoiceNo order by a.Amount desc\").show(5)","user":"anonymous","dateUpdated":"2020-03-09T15:25:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"invoiceInfo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, StockCode: string ... 6 more fields]\nlargeInvoiceInfo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double ... 3 more fields]\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\nonly showing top 5 rows\n\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=25","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=26","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=27"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151400_229833541","id":"20190520-122626_1736024345","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:04:12+0000","dateFinished":"2020-03-09T14:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:415"},{"text":"%md\n#### Q3: For each country, find the top N largest invoices by the amount and show receipt details\n\nUse `Window functions` and `rank()` function\n\nReadings:\n- https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n- https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\n- `Spark The Definitive Guide - page 134 - Windows Function`\n\n```\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n```","user":"anonymous","dateUpdated":"2020-03-09T15:39:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q3: For each country, find the top N largest invoices by the amount and show receipt details</h4>\n<p>Use <code>Window functions</code> and <code>rank()</code> function</p>\n<p>Readings:<br/>- <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br/>- <a href=\"https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\">https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a><br/>- <code>Spark The Definitive Guide - page 134 - Windows Function</code></p>\n<pre><code>+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151400_-1266990323","id":"20190520-150543_915955507","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T15:39:34+0000","dateFinished":"2020-03-09T15:39:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:416"},{"text":"//Obtain top two Invoice price totals per country and output Invoice info. Add a rank (per country) column to our Largest Invoice Info table\n//Then filter that table to only include 2 most expensive invoices per country\nval rankedInvoices = largeInvoiceInfo.withColumn(\"rank\", expr(\"rank()over(partition by Country order by Amount desc)\"))\nrankedInvoices.select('InvoiceNo, 'Amount, 'InvoiceDate, 'CustomerID, 'Country).where('rank <= 2).show(10)\n//Generate the ranking of most expensive invoice per country, then filter out the rank column by using the query as a subquery\nspark.sql(\"select InvoiceNo, Amount, InvoiceDate, CustomerID, Country from (select *, rank()over(partition by Country order by Amount desc) as rank from (select InvoiceNo, sum(Quantity * UnitPrice) as Amount, InvoiceDate, CustomerID, Country from retail group by InvoiceNo, InvoiceDate, CustomerID, Country) having rank < 3)\").show(10)","user":"anonymous","dateUpdated":"2020-03-09T15:29:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rankedInvoices: org.apache.spark.sql.DataFrame = [InvoiceNo: string, Amount: double ... 4 more fields]\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\nonly showing top 10 rows\n\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\nonly showing top 10 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=28","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=29","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=30","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=31","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=32","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=33","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=34","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=35","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=36","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=37"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151401_-1406138696","id":"20190520-125029_1350468290","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:05:07+0000","dateFinished":"2020-03-09T14:05:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:417"},{"text":"%md\n\n#### Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.\n\n\n```bash\ndailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n```\n\n```bash\nweeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n```\n\nReadings\n- https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/","user":"anonymous","dateUpdated":"2020-03-09T14:05:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.</h4>\n<pre><code class=\"bash\">dailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n</code></pre>\n<pre><code class=\"bash\">weeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n</code></pre>\n<p>Readings<br/>- <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1583437151402_1651822662","id":"20190520-140931_1510736707","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:05:44+0000","dateFinished":"2020-03-09T14:05:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:418"},{"text":"//Daily Sales amount\n//Group data by a 1-day sliding window, get the sum\n//of sales for the day, and show the start of the time window\nval dailyTotals = retailCastDf.groupBy(window('InvoiceDate, \"1 day\").getField(\"start\").cast(\"date\").as(\"Day\")).agg(sum('Quantity * 'UnitPrice).as(\"Sales_Amount\")).orderBy('Day)\ndailyTotals.show(5)\n//Make temp view for the Zeppelin graph to query\ndailyTotals.createOrReplaceTempView(\"dailySales\")\n//Strip the time from InvoiceDate, get the daily sales total, and group by day.\nspark.sql(\"select date(InvoiceDate) as Day, sum(Quantity * UnitPrice) as Sales_Amount from retail group by Day order by Day\").show(5)","user":"anonymous","dateUpdated":"2020-03-09T15:38:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":204,"optionOpen":false}}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dailyTotals: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Day: date, Sales_Amount: double]\n+----------+------------------+\n|       Day|      Sales_Amount|\n+----------+------------------+\n|2010-12-01| 58635.56000000026|\n|2010-12-02| 46207.27999999991|\n|2010-12-03|  45620.4599999999|\n|2010-12-05| 31383.95000000016|\n|2010-12-06|53860.180000000015|\n+----------+------------------+\nonly showing top 5 rows\n\n+----------+------------------+\n|       Day|      Sales_Amount|\n+----------+------------------+\n|2010-12-01| 58635.56000000026|\n|2010-12-02| 46207.27999999991|\n|2010-12-03|  45620.4599999999|\n|2010-12-05| 31383.95000000016|\n|2010-12-06|53860.180000000015|\n+----------+------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=48","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=49"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151403_-944808317","id":"20190520-181045_1661878813","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:10:12+0000","dateFinished":"2020-03-09T14:10:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:419"},{"text":"%sql\n-- Plot daily diagram here\nselect * from dailySales","user":"anonymous","dateUpdated":"2020-03-09T15:41:38+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":326,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"Day":"string","Sales_Amount":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{}},"commonSetting":{},"keys":[{"name":"Day","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Sales_Amount","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Day\tSales_Amount\n2010-12-01\t58635.56000000026\n2010-12-02\t46207.27999999991\n2010-12-03\t45620.4599999999\n2010-12-05\t31383.95000000016\n2010-12-06\t53860.180000000015\n2010-12-07\t45059.05000000015\n2010-12-08\t44189.839999999866\n2010-12-09\t52532.13000000003\n2010-12-10\t57404.91000000017\n2010-12-12\t17240.92000000005\n2010-12-13\t35379.34000000009\n2010-12-14\t42843.290000000205\n2010-12-15\t29443.69000000013\n2010-12-16\t48334.34999999993\n2010-12-17\t43534.19000000001\n2010-12-19\t7517.309999999992\n2010-12-20\t24741.750000000015\n2010-12-21\t47097.939999999915\n2010-12-22\t6134.569999999999\n2010-12-23\t11796.310000000025\n2011-01-04\t14950.480000000014\n2011-01-05\t-1566.2299999999464\n2011-01-06\t37392.73999999999\n2011-01-07\t27233.140000000065\n2011-01-09\t15710.800000000025\n2011-01-10\t24191.64\n2011-01-11\t67817.1300000002\n2011-01-12\t23958.77999999998\n2011-01-13\t20533.540000000005\n2011-01-14\t47377.2600000001\n2011-01-16\t7116.609999999981\n2011-01-17\t29256.000000000295\n2011-01-18\t18680.800000000003\n2011-01-19\t25585.81000000013\n2011-01-20\t17995.909999999974\n2011-01-21\t31978.440000000195\n2011-01-23\t10285.94999999997\n2011-01-24\t25555.620000000134\n2011-01-25\t27971.520000000113\n2011-01-26\t19493.320000000065\n2011-01-27\t21092.140000000076\n2011-01-28\t18567.76999999999\n2011-01-30\t6456.439999999991\n2011-01-31\t22364.650000000103\n2011-02-01\t28433.220000000023\n2011-02-02\t21048.45000000002\n2011-02-03\t23344.580000000096\n2011-02-04\t24994.170000000042\n2011-02-06\t3457.1099999999965\n2011-02-07\t25525.989999999983\n2011-02-08\t20728.140000000043\n2011-02-09\t16692.579999999976\n2011-02-10\t13427.539999999994\n2011-02-11\t20387.279999999984\n2011-02-13\t5535.399999999994\n2011-02-14\t26222.03000000017\n2011-02-15\t36842.58000000006\n2011-02-16\t24730.81000000009\n2011-02-17\t26361.87000000019\n2011-02-18\t15928.399999999987\n2011-02-20\t9578.890000000032\n2011-02-21\t23807.830000000176\n2011-02-22\t32292.62000000012\n2011-02-23\t26792.76000000015\n2011-02-24\t22655.830000000096\n2011-02-25\t18029.840000000007\n2011-02-27\t9491.049999999981\n2011-02-28\t21753.680000000186\n2011-03-01\t25471.71000000005\n2011-03-02\t18296.45\n2011-03-03\t35842.62000000009\n2011-03-04\t19474.8700000001\n2011-03-06\t9596.229999999987\n2011-03-07\t30525.58000000021\n2011-03-08\t25017.47000000018\n2011-03-09\t21907.120000000094\n2011-03-10\t25597.890000000112\n2011-03-11\t21995.28000000005\n2011-03-13\t4137.619999999995\n2011-03-14\t25864.590000000062\n2011-03-15\t20660.029999999955\n2011-03-16\t21182.640000000065\n2011-03-17\t38804.25000000005\n2011-03-18\t16770.459999999995\n2011-03-20\t21980.640000000098\n2011-03-21\t16370.270000000037\n2011-03-22\t31312.350000000228\n2011-03-23\t24029.070000000094\n2011-03-24\t36562.1000000001\n2011-03-25\t30656.03000000007\n2011-03-27\t8979.979999999996\n2011-03-28\t19207.030000000002\n2011-03-29\t70531.46999999978\n2011-03-30\t31489.25000000014\n2011-03-31\t31004.08000000019\n2011-04-01\t24391.780000000057\n2011-04-03\t6878.0999999999985\n2011-04-04\t25073.020000000182\n2011-04-05\t28353.83000000014\n2011-04-06\t17279.350000000064\n2011-04-07\t18229.00000000011\n2011-04-08\t23299.140000000112\n2011-04-10\t9363.879999999972\n2011-04-11\t22110.310000000063\n2011-04-12\t25124.250000000084\n2011-04-13\t23898.200000000023\n2011-04-14\t35295.580000000096\n2011-04-15\t28327.130999999976\n2011-04-17\t12704.299999999994\n2011-04-18\t32185.610000000288\n2011-04-19\t23837.650000000183\n2011-04-20\t28239.390000000025\n2011-04-21\t31198.60000000008\n2011-04-26\t30585.540000000157\n2011-04-27\t25590.56000000002\n2011-04-28\t21241.900000000063\n2011-05-01\t6964.659999999997\n2011-05-03\t19617.860000000135\n2011-05-04\t27462.3000000001\n2011-05-05\t28750.650000000238\n2011-05-06\t35714.58000000002\n2011-05-08\t18808.920000000046\n2011-05-09\t26060.43000000019\n2011-05-10\t45564.119999999915\n2011-05-11\t33240.36000000021\n2011-05-12\t59911.969999999856\n2011-05-13\t30744.07000000008\n2011-05-15\t9924.280000000004\n2011-05-16\t25279.77000000014\n2011-05-17\t53603.82999999985\n2011-05-18\t34337.29000000008\n2011-05-19\t34348.750000000015\n2011-05-20\t26256.520000000062\n2011-05-22\t24205.370000000068\n2011-05-23\t30739.550000000083\n2011-05-24\t37028.910000000054\n2011-05-25\t24152.280000000075\n2011-05-26\t33208.590000000026\n2011-05-27\t28232.19000000009\n2011-05-29\t7208.299999999988\n2011-05-31\t21967.96000000007\n2011-06-01\t20191.200000000124\n2011-06-02\t32502.010000000162\n2011-06-03\t16750.999999999996\n2011-06-05\t25520.35000000011\n2011-06-06\t16791.39\n2011-06-07\t37644.30000000004\n2011-06-08\t42940.909999999814\n2011-06-09\t45515.75000000025\n2011-06-10\t22540.659999999985\n2011-06-12\t12483.85999999999\n2011-06-13\t20372.930000000055\n2011-06-14\t40211.93000000002\n2011-06-15\t46139.179999999906\n2011-06-16\t34131.730000000054\n2011-06-17\t20800.72000000004\n2011-06-19\t22360.010000000104\n2011-06-20\t33493.4000000001\n2011-06-21\t22730.010000000064\n2011-06-22\t21794.940000000053\n2011-06-23\t24273.31000000014\n2011-06-24\t8619.880000000001\n2011-06-26\t6175.169999999984\n2011-06-27\t16823.859999999993\n2011-06-28\t34704.64000000011\n2011-06-29\t21775.43\n2011-06-30\t43834.550000000076\n2011-07-01\t13171.82000000001\n2011-07-03\t5977.139999999991\n2011-07-04\t44154.75000000023\n2011-07-05\t40334.9700000001\n2011-07-06\t26279.580000000133\n2011-07-07\t31357.72000000012\n2011-07-08\t26840.08000000012\n2011-07-10\t5692.069999999985\n2011-07-11\t22429.530000000123\n2011-07-12\t25892.040000000077\n2011-07-13\t11612.049999999948\n2011-07-14\t32575.960000000163\n2011-07-15\t14478.929999999933\n2011-07-17\t17174.660000000036\n2011-07-18\t28443.27000000024\n2011-07-19\t49316.780000000035\n2011-07-20\t27305.41000000014\n2011-07-21\t30957.06999999999\n2011-07-22\t20015.229999999978\n2011-07-24\t26476.200000000044\n2011-07-25\t26687.650000000176\n2011-07-26\t21271.30100000004\n2011-07-27\t25568.450000000106\n2011-07-28\t55706.880000000034\n2011-07-29\t18094.209999999955\n2011-07-31\t33486.359999999986\n2011-08-01\t21362.840000000022\n2011-08-02\t14947.269999999933\n2011-08-03\t27075.02000000017\n2011-08-04\t61028.6500000001\n2011-08-05\t21298.300000000094\n2011-08-07\t7464.1199999999935\n2011-08-08\t19987.149999999994\n2011-08-09\t26623.20000000008\n2011-08-10\t27474.220000000118\n2011-08-11\t72132.79000000005\n2011-08-12\t10049.47999999996\n2011-08-14\t5150.179999999998\n2011-08-15\t17205.54\n2011-08-16\t19103.710000000032\n2011-08-17\t49392.21999999998\n2011-08-18\t53225.669999999955\n2011-08-19\t17248.539999999994\n2011-08-21\t14549.210000000008\n2011-08-22\t27978.41000000003\n2011-08-23\t25756.30000000006\n2011-08-24\t37074.900000000045\n2011-08-25\t22458.880000000034\n2011-08-26\t25550.229999999978\n2011-08-28\t10784.779999999977\n2011-08-30\t31640.900000000096\n2011-08-31\t16117.999999999998\n2011-09-01\t37296.60000000006\n2011-09-02\t41745.07000000001\n2011-09-04\t17018.489999999998\n2011-09-05\t36844.04000000002\n2011-09-06\t28052.62000000006\n2011-09-07\t34125.65000000011\n2011-09-08\t26708.000000000106\n2011-09-09\t29317.690000000068\n2011-09-11\t35465.470000000096\n2011-09-12\t29039.310000000096\n2011-09-13\t54828.45000000007\n2011-09-14\t23360.660000000134\n2011-09-15\t62943.80999999994\n2011-09-16\t25858.060000000034\n2011-09-18\t15692.330000000029\n2011-09-19\t46212.21000000002\n2011-09-20\t109286.20999999993\n2011-09-21\t42944.07000000001\n2011-09-22\t57076.83000000002\n2011-09-23\t39426.48000000005\n2011-09-25\t31210.921000000104\n2011-09-26\t28642.27100000011\n2011-09-27\t35752.159999999916\n2011-09-28\t43383.03999999988\n2011-09-29\t43464.33000000008\n2011-09-30\t43992.84999999993\n2011-10-02\t11623.580000000014\n2011-10-03\t64214.78\n2011-10-04\t48240.839999999895\n2011-10-05\t75244.42999999986\n2011-10-06\t55306.27999999994\n2011-10-07\t47538.019999999815\n2011-10-09\t11922.239999999993\n2011-10-10\t44265.89000000007\n2011-10-11\t38267.75000000006\n2011-10-12\t29302.850000000173\n2011-10-13\t37067.17000000006\n2011-10-14\t35225.53999999997\n2011-10-16\t21605.440000000053\n2011-10-17\t47064.139999999956\n2011-10-18\t44637.84000000008\n2011-10-19\t36003.43000000002\n2011-10-20\t60793.13999999962\n2011-10-21\t62961.25999999998\n2011-10-23\t12302.410000000013\n2011-10-24\t38407.71999999994\n2011-10-25\t40807.49000000003\n2011-10-26\t37842.07999999995\n2011-10-27\t47480.1499999999\n2011-10-28\t39559.47000000007\n2011-10-30\t34545.28000000011\n2011-10-31\t48475.450000000164\n2011-11-01\t28741.55000000017\n2011-11-02\t45239.059999999816\n2011-11-03\t62816.54999999997\n2011-11-04\t60081.75999999989\n2011-11-06\t42912.400000000176\n2011-11-07\t70001.08000000009\n2011-11-08\t56647.659999999814\n2011-11-09\t62599.42999999966\n2011-11-10\t68956.24000000003\n2011-11-11\t54835.509999999995\n2011-11-13\t33520.22000000013\n2011-11-14\t112141.10999999996\n2011-11-15\t60594.229999999865\n2011-11-16\t64408.7000000001\n2011-11-17\t60329.719999999776\n2011-11-18\t48031.80000000006\n2011-11-20\t34902.01000000018\n2011-11-21\t48302.499999999796\n2011-11-22\t62307.31999999994\n2011-11-23\t78480.6999999997\n2011-11-24\t48080.279999999926\n2011-11-25\t50442.72000000002\n2011-11-27\t20571.500000000084\n2011-11-28\t55442.01999999994\n2011-11-29\t72219.19999999998\n2011-11-30\t59150.97999999988\n2011-12-01\t51410.94999999973\n2011-12-02\t57086.059999999874\n2011-12-04\t24565.78000000009\n2011-12-05\t57751.31999999973\n2011-12-06\t54228.37000000012\n2011-12-07\t75076.21999999967\n2011-12-08\t81417.77999999982\n2011-12-09\t32131.53000000001\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=50"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151403_1172843788","id":"20190520-140933_785400989","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T14:10:24+0000","dateFinished":"2020-03-09T14:10:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:420"},{"text":"//Weekly total sales\n//Group by sliding time window, show start of week\n//Sum up weekly sales, and order by week. For this query we assume the first week starts on 2010-11-29, the monday before the first record.\nval weeklyTotals = retailCastDf.groupBy(window('InvoiceDate, \"1 week\", \"1 week\", \"4 days\").getField(\"start\").cast(\"date\").as(\"Week\")).agg(sum('Quantity * 'UnitPrice).as(\"Total_Sales\")).orderBy('Week)\nweeklyTotals.show()\n//Make temp view for the Zeppelin graph to query\nweeklyTotals.createOrReplaceTempView(\"weeklySales\")\n//Verification using SparkSQL\n/*Get 1/7th of the days between the start of the first week and the invoice date\n  to group by week. Then get the sum of sales for that week. The first week is\n  assumed to start on 2010-11-29. Query that result to filter out the datediff column\n*/\nspark.sql(\"select WeekStart, WeeklyTotal from (select date(first(InvoiceDate))as WeekStart, cast(datediff(InvoiceDate, '2010-11-29')/7 as Int) as Week, sum(Quantity * UnitPrice) as WeeklyTotal from retail group by Week) order by Week\").show()","user":"anonymous","dateUpdated":"2020-03-09T15:38:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"weeklyTotals: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Week: date, Total_Sales: double]\n+----------+------------------+\n|      Week|       Total_Sales|\n+----------+------------------+\n|2010-11-29|181847.25000000538|\n|2010-12-06|270287.03000000614|\n|2010-12-13|207052.17000000254|\n|2010-12-20| 89770.57000000049|\n|2011-01-03| 93720.92999999932|\n|2011-01-10|190994.96000000346|\n|2011-01-17|133782.90999999916|\n|2011-01-24|119136.80999999883|\n|2011-01-31|123642.17999999887|\n|2011-02-07|102296.92999999903|\n|2011-02-14| 139664.5800000008|\n|2011-02-21|133069.92999999927|\n|2011-02-28|130435.55999999912|\n|2011-03-07|129180.95999999889|\n|2011-03-14|145262.60999999987|\n|2011-03-21|147909.79999999917|\n|2011-03-28|183501.71000000107|\n|2011-04-04|121598.21999999865|\n|2011-04-11|147459.77100000044|\n|2011-04-18|115461.24999999964|\n+----------+------------------+\nonly showing top 20 rows\n\n+----------+------------------+\n| WeekStart|       WeeklyTotal|\n+----------+------------------+\n|2010-12-01|181847.25000000538|\n|2010-12-06|270287.03000000614|\n|2010-12-13|207052.17000000254|\n|2010-12-20| 89770.57000000049|\n|2011-01-04| 93720.92999999932|\n|2011-01-10|190994.96000000346|\n|2011-01-17|133782.90999999916|\n|2011-01-24|119136.80999999883|\n|2011-01-31|123642.17999999887|\n|2011-02-07|102296.92999999903|\n|2011-02-14| 139664.5800000008|\n|2011-02-21|133069.92999999927|\n|2011-02-28|130435.55999999912|\n|2011-03-07|129180.95999999889|\n|2011-03-14|145262.60999999987|\n|2011-03-21|147909.79999999917|\n|2011-03-28|183501.71000000107|\n|2011-04-04|121598.21999999865|\n|2011-04-11|147459.77100000044|\n|2011-04-18|115461.24999999964|\n+----------+------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=126","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=127"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151405_1783330684","id":"20190520-140933_428817963","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T15:05:04+0000","dateFinished":"2020-03-09T15:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:421"},{"text":"%sql\n-- Plot weekly diagram here\nselect * from weeklySales","user":"anonymous","dateUpdated":"2020-03-09T15:41:39+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"to_date(weeklysales.`start`)":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"Week","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Total_Sales","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Week\tTotal_Sales\n2010-11-29\t181847.25000000538\n2010-12-06\t270287.03000000614\n2010-12-13\t207052.17000000254\n2010-12-20\t89770.57000000049\n2011-01-03\t93720.92999999932\n2011-01-10\t190994.96000000346\n2011-01-17\t133782.90999999916\n2011-01-24\t119136.80999999883\n2011-01-31\t123642.17999999887\n2011-02-07\t102296.92999999903\n2011-02-14\t139664.5800000008\n2011-02-21\t133069.92999999927\n2011-02-28\t130435.55999999912\n2011-03-07\t129180.95999999889\n2011-03-14\t145262.60999999987\n2011-03-21\t147909.79999999917\n2011-03-28\t183501.71000000107\n2011-04-04\t121598.21999999865\n2011-04-11\t147459.77100000044\n2011-04-18\t115461.24999999964\n2011-04-25\t84382.65999999993\n2011-05-02\t130354.31000000057\n2011-05-09\t205445.2300000001\n2011-05-16\t198031.5300000006\n2011-05-23\t160569.82000000172\n2011-05-30\t116932.51999999893\n2011-06-06\t177916.8699999994\n2011-06-13\t184016.49999999983\n2011-06-20\t117086.70999999905\n2011-06-27\t136287.4400000001\n2011-07-04\t174659.17000000188\n2011-07-11\t124163.16999999889\n2011-07-18\t182513.9599999996\n2011-07-25\t180814.85099999997\n2011-08-01\t153176.20000000022\n2011-08-08\t161417.0200000002\n2011-08-15\t170724.88999999952\n2011-08-22\t149603.50000000073\n2011-08-29\t143819.06000000067\n2011-09-05\t190513.47000000073\n2011-09-12\t211722.61999999965\n2011-09-19\t326156.72100000986\n2011-09-26\t206858.23099999997\n2011-10-03\t302466.5900000035\n2011-10-10\t205734.640000002\n2011-10-17\t263762.2200000022\n2011-10-24\t238642.19000000117\n2011-10-31\t288266.77000000695\n2011-11-07\t346560.14000001503\n2011-11-14\t380407.57000001665\n2011-11-21\t308185.0200000139\n2011-11-28\t319874.99000000855\n2011-12-05\t300605.220000006\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=128"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1583437151406_-255068226","id":"20190520-212256_1274740776","dateCreated":"2020-03-05T19:39:11+0000","dateStarted":"2020-03-09T15:06:25+0000","dateFinished":"2020-03-09T15:06:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:422"}],"name":"Jarvis/2-DataFrame_pub","id":"2F3GB7VH2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}