{"paragraphs":[{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a existing Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and Spark Session\n[How to use SparkSession](https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html)\n[Spark Documentation - SparkSession](https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession) \n[Spark Documentation - SparkContext](https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext)\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zepplin will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a existing Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and Spark Session<br/><a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">How to use SparkSession</a><br/><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">Spark Documentation - SparkSession</a><br/><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">Spark Documentation - SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zepplin will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311544_469306303","id":"20190921-014743_1530188134","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:39+0000","dateFinished":"2020-03-04T17:07:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:286"},{"text":"//Spark session and sparkContext are loaded automatically\nprintln(s\"Spark Version: ${spark.version.to}\")\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Spark Version: 2.3.4\norg.apache.spark.sql.SparkSession@388bac32\norg.apache.spark.SparkContext@2abe99fc\norg.apache.spark.SparkContext@2abe99fc\n"}]},"apps":[],"jobName":"paragraph_1582576311548_-1712801268","id":"20190921-013657_404311467","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:39+0000","dateFinished":"2020-03-04T17:07:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"text":"%md\n### Comparing SparkSession and SparkContext\nThe SparkSession, referred to simply with `spark`, is the overarching class for interacting with Spark. You can have only one SparkSession per application. The SparkSession is the driver which allows Scala to interact with a Spark cluster. SparkSession provides access to many other spark classes and packages.\nThe SparkContext acts as the connection between Scala and the Spark cluster. There can only be one SparkContext per application, as it belongs to the Session. The SparkContext can be used to create RDDs, from files or Scala data, and accumulators.","user":"anonymous","dateUpdated":"2020-03-04T17:07:40+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Comparing SparkSession and SparkContext</h3>\n<p>The SparkSession, referred to simply with <code>spark</code>, is the overarching class for interacting with Spark. You can have only one SparkSession per application. The SparkSession is the driver which allows Scala to interact with a Spark cluster. SparkSession provides access to many other spark classes and packages.<br/>The SparkContext acts as the connection between Scala and the Spark cluster. There can only be one SparkContext per application, as it belongs to the Session. The SparkContext can be used to create RDDs, from files or Scala data, and accumulators.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311548_-80664894","id":"20190922-220218_788870347","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:40+0000","dateFinished":"2020-03-04T17:07:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2020-03-04T17:07:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311549_1104039755","id":"20190921-022812_325072599","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:40+0000","dateFinished":"2020-03-04T17:07:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2020-03-04T17:07:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[265] at parallelize at <console>:33\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[266] at flatMap at <console>:35\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=117","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=118","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=119","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=120"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311550_-1713800196","id":"20190921-020350_225494359","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:40+0000","dateFinished":"2020-03-04T17:07:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"text":"","user":"anonymous","dateUpdated":"2020-03-04T17:07:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311550_258213269","id":"20190922-220230_613999600","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD","user":"anonymous","dateUpdated":"2020-03-04T17:07:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311551_2006274885","id":"20190920-182511_1653833929","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:42+0000","dateFinished":"2020-03-04T17:07:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"// Create a RDD from a file\n// Our file is a plaintext CSV stored on hadoop\nval retailRDD = sc.textFile(\"hdfs:///user/vassairm/datasets/online_retail/online-retail-dataset.txt\")\n\n//count number of elements in the RDD\nretailRDD.count\n\n//understand what does each element look like in RDD\nval firstE = retailRDD.first()\n\n//find out does withReplacement mean from the scala doc https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\nretailRDD.takeSample(false, 5).foreach(println)","user":"anonymous","dateUpdated":"2020-03-04T17:07:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/vassairm/datasets/online_retail/online-retail-dataset.txt MapPartitionsRDD[269] at textFile at <console>:34\nres419: Long = 541910\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n581498,71053,WHITE MOROCCAN METAL LANTERN,1,12/9/2011 10:26,8.29,,United Kingdom\n564172,22277,COSMETIC BAG VINTAGE ROSE PAISLEY,3,8/23/2011 14:19,2.1,16033,United Kingdom\n574040,22835,HOT WATER BOTTLE I AM SO POORLY,1,11/2/2011 13:01,4.95,14974,United Kingdom\n557130,21977,PACK OF 60 PINK PAISLEY CAKE CASES,24,6/17/2011 8:09,0.55,14005,United Kingdom\n567667,23158,SET OF 5 LUCKY CAT MAGNETS ,1,9/21/2011 15:24,4.13,,United Kingdom\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=121","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=122","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=123","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=124"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311552_1077743817","id":"20190920-182724_1961848616","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:42+0000","dateFinished":"2020-03-04T17:07:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:293"},{"text":"%md\n### CSV format issue\n- With our current parsing scheme of simply splitting fields by commas, some records appear to contain more columns than usual. This is due to the fact that certain fields contain non-splitting commas, primarily the product description field, but some countries may contain commas as well.\n- Possible solutions include writing a more complex mapping function that can handle csv edge cases, or we can use an alternate Spark method to load the file directly into a DataFrame using `spark.read.format(\"csv\").load(\"/path/to/data.file\")`. A third alternative is to pre-process our data to eliminate edge cases from working with CSVs.","user":"anonymous","dateUpdated":"2020-03-04T17:07:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<ul>\n  <li>With our current parsing scheme of simply splitting fields by commas, some records appear to contain more columns than usual. This is due to the fact that certain fields contain non-splitting commas, primarily the product description field, but some countries may contain commas as well.</li>\n  <li>Possible solutions include writing a more complex mapping function that can handle csv edge cases, or we can use an alternate Spark method to load the file directly into a DataFrame using <code>spark.read.format(&quot;csv&quot;).load(&quot;/path/to/data.file&quot;)</code>. A third alternative is to pre-process our data to eliminate edge cases from working with CSVs.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311553_-1317737749","id":"20190921-023538_989684097","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:44+0000","dateFinished":"2020-03-04T17:07:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:294"},{"text":"//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, cols) => println(count + \":\" + cols.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"splitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[271] at map at <console>:35\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[274] at map at <console>:37\n9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=125","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=126","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=127"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311553_-1344941882","id":"20190921-023311_1509488233","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:44+0000","dateFinished":"2020-03-04T17:07:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"seond, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt > online-retail-dataset_clean.txt`\n- Remove all double double quotes<br>`sed -i 's/\"//g' online-retail-dataset_clean.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to HDFS\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2020-03-04T17:07:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;seond, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt &gt; online-retail-dataset_clean.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed -i &#39;s/&quot;//g&#39; online-retail-dataset_clean.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to HDFS</p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311554_-541217307","id":"20190519-113048_765206384","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:46+0000","dateFinished":"2020-03-04T17:07:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"text":"//Load csv file\n//Lazy evaluation\nval filePath = \"hdfs:///user/vassairm/datasets/online_retail/online-retail-dataset_clean.txt\"\n// Filter out the line with the column headers. This way we wont have to always skip the first line\nval retailRDD = sc.textFile(filePath).filter(x => x.split(\",\")(0) != \"InvoiceNo\")\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//Another RDD action\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 8 columns, count here should be 0\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2020-03-04T17:07:46+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/vassairm/datasets/online_retail/online-retail-dataset_clean.txt\nretailRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[278] at filter at <console>:35\ncount: Long = 541909\nsample3: Array[String] = Array(569457,22329,ROUND CONTAINER SET OF 5 RETROSPOT,1,10/4/2011 11:29,1.65,14606,United Kingdom, 571265,22530,MAGIC DRAWING SLATE DOLLY GIRL ,2,10/16/2011 11:31,0.42,16674,United Kingdom, 563893,90064B,BLACK VINTAGE  CRYSTAL EARRINGS,1,8/19/2011 17:10,3.75,16330,United Kingdom)\nlongRow: Long = 0\nres436: retailRDD.type = MapPartitionsRDD[278] at filter at <console>:35\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=128","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=129","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=130","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=131"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311554_1554510692","id":"20190519-105016_1691323616","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:47+0000","dateFinished":"2020-03-04T17:07:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"//Utility functions for printing RDDs\nval printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\nval printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\nval printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:49+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"printRddNSamples: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nprintRdd3Samples: org.apache.spark.rdd.RDD[_] => Unit = <function1>\nprintRddTopN: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nbars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1582576311555_-194793615","id":"20190519-192640_1954412488","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:49+0000","dateFinished":"2020-03-04T17:07:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%md\n### Spark RDD Transormations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- Spark: The Definitive Guide Chapter 12 (required) & Chapter 13 (Optional)\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n_Note that the \"file\" saved by Spark is actually a directory containing the RDD's partitions and a_ _\\_SUCCESS file_\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{"0":{"graph":{"mode":"table","height":835.333,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transormations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li>Spark: The Definitive Guide Chapter 12 (required) &amp; Chapter 13 (Optional)</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n<p><em>Note that the &ldquo;file&rdquo; saved by Spark is actually a directory containing the RDD&rsquo;s partitions and a</em> <em>_SUCCESS file</em></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311555_-241540928","id":"20190519-115905_2023471169","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:51+0000","dateFinished":"2020-03-04T17:07:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"text":"// Print out the first value\nprintMsg(\"RDD Actions #1\")\nprintln(retailRDD.first)\n\n// Take the first 5 values and print them out\nprintMsg(\"RDD Actions #2\")\nretailRDD.take(5).foreach(println)\n\n// Get all records from the RDD and print the count\nprintMsg(\"RDD Actions #3\")\nprintln(retailRDD.collect.length)\n\n// Get 5 random elements from the RDD\nprintMsg(\"RDD Actions #4\")\nretailRDD.takeSample(false, 5, 10).foreach(println)\n\n// Save all elements from the RDD to HDFS as plain text files.\n// Using saveAsTextFile() creates a directory named \"text.txt\" which contains two partitions\nprintMsg(\"RDD Actions #5\")\nprintln(\"Saving RDD as '/tmp/text.txt'\")\n// Because of Hadoop/HDFS design philosophy, this will throw an exception if the dir already exists.\n// Overwiritng files can lead to data loss, so it's generally not allowed.\nretailRDD.saveAsTextFile(\"hdfs:///tmp/text.txt\")","user":"anonymous","dateUpdated":"2020-03-04T17:07:51+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":450.45,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------RDD Actions #1---------\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n---------RDD Actions #2---------\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n---------RDD Actions #3---------\n541909\n---------RDD Actions #4---------\n553393,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,5/16/2011 16:46,8.29,,United Kingdom\n538148,DOT,DOTCOM POSTAGE,1,12/9/2010 16:26,547.32,,United Kingdom\n539243,22834,HAND WARMER BABUSHKA DESIGN,3,12/16/2010 13:21,2.1,18116,United Kingdom\n548308,21124,SET/10 BLUE POLKADOT PARTY CANDLES,5,3/30/2011 12:00,1.25,17220,United Kingdom\n545067,21754,HOME BUILDING BLOCK WORD,2,2/27/2011 14:49,5.95,17894,United Kingdom\n---------RDD Actions #5---------\nSaving RDD as '/tmp/text.txt'\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=132","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=133","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=134","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=135","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=136","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=137"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311556_1979873917","id":"20190519-122034_629713430","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:51+0000","dateFinished":"2020-03-04T17:07:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"user":"anonymous","dateUpdated":"2020-03-04T17:07:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","tableHide":true,"editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583337718321_1702821604","id":"20200304-160158_1128914240","dateCreated":"2020-03-04T16:01:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use fileter)\n2. Compare `sample` and `takeSample`\n\nSample outputs:\n```bash\n#1\nSales from United Kingdom: 495478\n#2\nSample size with RDD.sample(25%): 135398\nSample size with RDD.takeSample(25): 25\n```","user":"anonymous","dateUpdated":"2020-03-04T17:07:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use fileter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\nSales from United Kingdom: 495478\n#2\nSample size with RDD.sample(25%): 135398\nSample size with RDD.takeSample(25): 25\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311557_1596140656","id":"20190917-181850_863623231","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:55+0000","dateFinished":"2020-03-04T17:07:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:302"},{"text":"// Get all sales from the UK\nprintMsg(\"RDD Transformations #1\")\nval ukRDD = retailRDD.map(row => row.split(\",\")).filter(record => record(7) equals \"United Kingdom\")\nprintln(\"Sales from United Kingdom: \" + ukRDD.count)\n// Compare sample() and takeSample()\nprintMsg(\"RDD Transformations #2\")\n// RDD.sample() returns an RDD[T]\nprintln(s\"Sample size with RDD.sample(25%): ${retailRDD.sample(false, 0.25).count}\")\n// RDD.takeSample() returns an Array[T]\nprintln(s\"Sample size with RDD.takeSample(25): ${retailRDD.takeSample(false, 25).length}\")","user":"anonymous","dateUpdated":"2020-03-04T17:07:55+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------RDD Transformations #1---------\nukRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[284] at filter at <console>:35\nSales from United Kingdom: 495478\n---------RDD Transformations #2---------\nSample size with RDD.sample(25%): 136023\nSample size with RDD.takeSample(25): 25\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=138","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=139","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=140","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=141"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311558_120729168","id":"20190519-124053_1683164197","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:55+0000","dateFinished":"2020-03-04T17:07:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2020-03-04T17:07:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311558_868687654","id":"20190519-125017_38292448","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:56+0000","dateFinished":"2020-03-04T17:07:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"text":"%md\n\n#### Questions 1.0\n\nTransform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//where (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2020-03-04T17:11:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Transform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//where (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311559_-1799606262","id":"20190519-195132_1947538683","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:11:06+0000","dateFinished":"2020-03-04T17:11:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"//Qustion 1.0 soutuion\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val quantity = tokens(3)\n    val unitPrice = tokens(5)\n    val amount = quantity.toInt * unitPrice.toDouble\n    (country, amount)\n}\n\nprintMsg(\"#1.0\")\nval ctryRdd = retailRDD.map(parseKeyValue)\nctryRdd.takeSample(false, 3)","user":"anonymous","dateUpdated":"2020-03-04T17:07:57+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parseKeyValue: String => (String, Double) = <function1>\n---------#1.0---------\nctryRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[287] at map at <console>:37\nres472: Array[(String, Double)] = Array((United Kingdom,6.36), (United Kingdom,4.95), (United Kingdom,20.8))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=142","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=143"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311560_-787072043","id":"20190519-125921_348001552","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:57+0000","dateFinished":"2020-03-04T17:07:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"user":"anonymous","dateUpdated":"2020-03-04T17:07:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311560_-279756828","id":"20190922-215540_2030793994","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```","user":"anonymous","dateUpdated":"2020-03-04T17:09:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311561_-1360608105","id":"20190519-195238_1235609517","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:09:21+0000","dateFinished":"2020-03-04T17:09:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"text":"printMsg(\"#1.1\")\n// Get total sales per country, sort descending\nval totalSales = ctryRdd.reduceByKey((x: Double, y: Double) => x + y).sortBy(_._2, ascending = false)\n// We take here instead of using takeOrdered, top, or sample to show that the sort worked\ntotalSales.take(5).foreach(println)\n","user":"anonymous","dateUpdated":"2020-03-04T17:07:59+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------#1.1---------\ntotalSales: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[294] at sortBy at <console>:41\n(United Kingdom,8187806.363998696)\n(Netherlands,284661.539999999)\n(EIRE,263276.81999999884)\n(Germany,221698.21000000017)\n(France,197403.90000000026)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=144","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=145"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311561_504980277","id":"20190519-195236_1582695577","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:07:59+0000","dateFinished":"2020-03-04T17:08:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"user":"anonymous","dateUpdated":"2020-03-04T17:08:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311562_1209303350","id":"20190922-215553_1555963294","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n","user":"anonymous","dateUpdated":"2020-03-04T17:09:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311562_-1752191812","id":"20190519-195157_1405617071","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:09:51+0000","dateFinished":"2020-03-04T17:09:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"// Filter out lines with no Customer ID, then\n// generate an RDD of the form (country, customer ID)\nval idRDD = retailRDD.filter(x => x.split(\",\")(6) != \"\").map(x => {\n    val cols = x.split(\",\")\n    val country = cols.last\n    val customerId = cols(6).toInt\n    (country, customerId)\n})\n\nprintMsg(\"#2.0\")\n// Get the smallest customer ID for each country, sort ascending by customer ID\nval earliestCustomer = idRDD.reduceByKey((x: Int, y: Int) => if (x < y) x else y).sortBy(_._2, ascending = true)\nearliestCustomer.take(5).foreach(println)","user":"anonymous","dateUpdated":"2020-03-04T17:08:01+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true,"lineNumbers":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"idRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[296] at map at <console>:38\n---------#2.0---------\nearliestCustomer: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[302] at sortBy at <console>:39\n(United Kingdom,12346)\n(Iceland,12347)\n(Finland,12348)\n(Italy,12349)\n(Norway,12350)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=146","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=147"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311563_996693349","id":"20190519-144439_1578143376","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:01+0000","dateFinished":"2020-03-04T17:08:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:312"},{"user":"anonymous","dateUpdated":"2020-03-04T17:08:02+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311564_-849462416","id":"20190922-215609_1113478409","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we can convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n","user":"anonymous","dateUpdated":"2020-03-04T17:10:13+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we can convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311564_1150679310","id":"20190921-180308_1963749922","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:10:13+0000","dateFinished":"2020-03-04T17:10:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"text":"// Item case class definition\n// Some fields may be blank, so we treat those fields as Options\n// An empty string for a field is translated to None\ncase class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerId:Option[Int], country:String)\ndef rowToItem(row: String): Item = {\n    val cols = row.split(\",\")\n    val invoiceNo = cols(0)\n    val stockCode = cols(1)\n    // Description may be blank, treat that as None\n    val description = if (cols(2).equals(\"\")) None else Some(cols(2))\n    val quantity = cols(3).toInt\n    val invoiceDate = cols(4)\n    val unitPrice = cols(5).toDouble\n    // Customer ID may be blank, treat that as None\n    val customerId = if (cols(6).equals(\"\")) None else Some(cols(6).toInt)\n    val country = cols(7)\n    // Construct the Item with Item.apply()\n    Item(invoiceNo, stockCode, description, quantity, invoiceDate, unitPrice, customerId, country)\n}\n// Create the Item RDD by mapping rows of the Retail RDD to Items\nval itemRDD = retailRDD.map(rowToItem(_))\nprintMsg(\"#2.1\")\nitemRDD.takeSample(false,5).foreach(println)\n","user":"anonymous","dateUpdated":"2020-03-04T17:08:03+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\nrowToItem: (row: String)Item\nitemRDD: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[303] at map at <console>:41\n---------#2.1---------\nItem(565682,21340,Some(CLASSIC METAL BIRDCAGE PLANT HOLDER),1,9/6/2011 10:42,12.75,Some(14796),United Kingdom)\nItem(553912,82482,Some(WOODEN PICTURE FRAME WHITE FINISH),6,5/19/2011 19:32,2.55,Some(15122),United Kingdom)\nItem(571409,20704,Some(MR ROBOT SOFT TOY),4,10/17/2011 12:25,6.95,Some(14911),EIRE)\nItem(562024,22665,Some(RECIPE BOX BLUE SKETCHBOOK DESIGN),1,8/1/2011 16:07,5.79,None,United Kingdom)\nItem(541107,20992,Some(JAZZ HEARTS PURSE NOTEBOOK),12,1/13/2011 14:55,0.85,Some(17744),United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=148","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=149"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311565_-1511791203","id":"20190921-180433_1776305327","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:03+0000","dateFinished":"2020-03-04T17:08:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:315"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-03-04T17:08:04+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311565_-1730489573","id":"20190922-215634_337314214","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:316"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.0 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2020-03-04T17:08:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.0 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311566_1750661344","id":"20190922-140917_1244358721","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:05+0000","dateFinished":"2020-03-04T17:08:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:317"},{"text":"// Total sales per country descending\nprintMsg(\"1.1 Item Ver.\")\nval salesPerCountry2 = itemRDD.map(item => (item.country, item.quantity * item.unitPrice)).reduceByKey((x:Double, y:Double) => x + y).sortBy(_._2, ascending = false)\n// Print out first 5 Pairs to check sorting\nsalesPerCountry2.take(5).foreach(println)\n// Earliest customer ID per country\nprintMsg(\"2.0 Item Ver.\")\n// Filter out all Items with no customer ID, then map to KVPs, then reduce and sort\nval earliestCustomer2 = itemRDD.filter(x => x.customerId.nonEmpty).map(item => (item.country, item.customerId.get)).reduceByKey((x, y) => if (x < y) x else y).sortBy(_._2)\n// Print out first 5 Pairs to check sorting\nearliestCustomer2.take(5).foreach(println)","user":"anonymous","dateUpdated":"2020-03-04T17:08:05+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------1.1 Item Ver.---------\nsalesPerCountry2: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[311] at sortBy at <console>:41\n(United Kingdom,8187806.363998696)\n(Netherlands,284661.539999999)\n(EIRE,263276.81999999884)\n(Germany,221698.21000000017)\n(France,197403.90000000026)\n---------2.0 Item Ver.---------\nearliestCustomer2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[319] at sortBy at <console>:43\n(United Kingdom,12346)\n(Iceland,12347)\n(Finland,12348)\n(Italy,12349)\n(Norway,12350)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=150","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=151","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=152","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=153"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311567_437145898","id":"20190922-141025_1178356031","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:05+0000","dateFinished":"2020-03-04T17:08:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2020-03-04T17:08:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311568_644469757","id":"20190519-195407_556558321","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:07+0000","dateFinished":"2020-03-04T17:08:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"// Count unique customer IDs. \n// No ID is considered no customer and is excluded from the count\nval uniqueCustomers = itemRDD.filter(item => item.customerId.nonEmpty).map(item => item.customerId.get).distinct\nprintMsg(\"#3.0\")\nprintln(s\"Customer Count: ${uniqueCustomers.count}\")\n","user":"anonymous","dateUpdated":"2020-03-04T17:08:07+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"uniqueCustomers: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[324] at distinct at <console>:44\n---------#3.0---------\nCustomer Count: 4372\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=154"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311568_2096639837","id":"20190519-194831_1486531342","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:08+0000","dateFinished":"2020-03-04T17:08:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"user":"anonymous","dateUpdated":"2020-03-04T17:10:28+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1582576311569_-108075128","id":"20190922-215651_1847295545","dateCreated":"2020-02-24T20:31:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:321"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2020-03-04T17:08:09+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311569_60267435","id":"20190519-195310_661372203","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:09+0000","dateFinished":"2020-03-04T17:08:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:322"},{"text":"// Get invoice count per customer\n// Invoices with no Customer ID are ignored\n// AggregateByKey takes two functions: the first increments an accumulator per invoice number, and the second adds two accumulators together\nval numInvoicesPerCustomer = itemRDD.filter(item => item.customerId.nonEmpty).map(item => (item.customerId.get, item.invoiceNo)).aggregateByKey(0)((x:Int, y:String) => x+1, (a:Int, b) => a+b)\n\nprintMsg(\"#4.0\")\n// Print out 5 random customer IDs' order counts in the form (customer ID, order count)\nnumInvoicesPerCustomer.takeSample(false, 5).foreach(println)","user":"anonymous","dateUpdated":"2020-03-04T17:08:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"numInvoicesPerCustomer: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[327] at aggregateByKey at <console>:45\n---------#4.0---------\n(13260,8)\n(13149,86)\n(14670,132)\n(14014,39)\n(15351,233)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=155","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=156"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311570_1970782088","id":"20190922-190215_1478690977","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:09+0000","dateFinished":"2020-03-04T17:08:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"text":"%md\n#### SPARK UI\nEvery Spark Job has a web UI for monitroing and debuging purposes.\nGo to GCP > your hadoop cluster > web interfaces > Spark History Server > Spark UI. ","user":"anonymous","dateUpdated":"2020-03-04T17:08:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Every Spark Job has a web UI for monitroing and debuging purposes.<br/>Go to GCP &gt; your hadoop cluster &gt; web interfaces &gt; Spark History Server &gt; Spark UI.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311570_-112737030","id":"20190521-114127_1095254606","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:11+0000","dateFinished":"2020-03-04T17:08:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2020-03-04T17:08:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1582576311571_47710555","id":"20190519-183851_617743118","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:11+0000","dateFinished":"2020-03-04T17:08:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"text":"// Load customer data into a RDD[Customer]\nprintMsg(\"#5.1\")\ncase class Customer(customerId:Int, name: String)\n// Customers.txt is a CSV with no headers, so no pre-processing is needed\nval customerRaw = sc.textFile(\"hdfs:///user/vassairm/datasets/online_retail/customers.txt\")\n// Convert the raw text RDD into a Customer RDD\nval customerRDD = customerRaw.map (row => {\n    val values = row.split(\",\")\n    Customer(values(0).toInt, values(1))\n})\n\n// Join the RDD[Customer] with RDD[Item] on customerId\nprintMsg(\"#5.2\")\n// Key both RDDs by customer ID, then use PairRDDFunctions.leftOuterJoin() to join them\n// leftOuterJoin is used since there may be customers who have no invoices\n// Invoices with no customer are filtered out.\nval customerInvoiceRDD = customerRDD.keyBy(_.customerId).leftOuterJoin(itemRDD.filter(item => item.customerId.nonEmpty).keyBy(_.customerId.get))\n\n// Get all unique (customer ID, name, country) triplets\nprintMsg(\"#5.3\")\n// Map the joined RDD to the required triplet format, then get distinct triplets\nval customerInfoRDD = customerInvoiceRDD.map(x => {\n    val id = x._1\n    val customer = x._2._1\n    val item = x._2._2.getOrElse(Item(\"\",\"\",None,0,\"\",0.0,Some(id),\"N/A\"))\n    (id, customer.name, item.country)\n}).distinct\n// Print a sample to check format\ncustomerInfoRDD.takeSample(false, 5).foreach(println)","user":"anonymous","dateUpdated":"2020-03-04T17:08:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------#5.1---------\ndefined class Customer\ncustomerRaw: org.apache.spark.rdd.RDD[String] = hdfs:///user/vassairm/datasets/online_retail/customers.txt MapPartitionsRDD[330] at textFile at <console>:33\ncustomerRDD: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[331] at map at <console>:37\n---------#5.2---------\ncustomerInvoiceRDD: org.apache.spark.rdd.RDD[(Int, (Customer, Option[Item]))] = MapPartitionsRDD[337] at leftOuterJoin at <console>:51\n---------#5.3---------\ncustomerInfoRDD: org.apache.spark.rdd.RDD[(Int, String, String)] = MapPartitionsRDD[341] at distinct at <console>:56\n(14560, Eaton E. Ramos,United Kingdom)\n(12865, Abraham K. Fox,Austria)\n(14396, Alvin V. Ellison,United Kingdom)\n(13708, Wade P. Owen,United Kingdom)\n(13593, Remedios D. Brown,United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=157","http://jarvis-hadoop-hive-mv-m.us-east1-d.c.jarvis-cloud-rdp.internal:4040/jobs/job?id=158"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1582576311571_-1686702308","id":"20190922-193513_1093352183","dateCreated":"2020-02-24T20:31:51+0000","dateStarted":"2020-03-04T17:08:11+0000","dateFinished":"2020-03-04T17:08:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583341691548_1517397709","id":"20200304-170811_2057707468","dateCreated":"2020-03-04T17:08:11+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:327"}],"name":"Jarvis/1-SparkRDD_pub","id":"2F453EBNJ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}